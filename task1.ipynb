{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a41b5ce0-853c-4ee3-9511-114a0b83c31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import os\n",
    "import chardet\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d5f337c5-c15b-46b2-b004-6f4872ad7bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "posts/blackassign0001.txt\n",
      "posts/blackassign0002.txt\n",
      "posts/blackassign0003.txt\n",
      "posts/blackassign0004.txt\n",
      "posts/blackassign0005.txt\n",
      "posts/blackassign0006.txt\n",
      "posts/blackassign0007.txt\n",
      "posts/blackassign0008.txt\n",
      "posts/blackassign0009.txt\n",
      "posts/blackassign0010.txt\n",
      "posts/blackassign0011.txt\n",
      "posts/blackassign0012.txt\n",
      "posts/blackassign0013.txt\n",
      "posts/blackassign0014.txt\n",
      "posts/blackassign0015.txt\n",
      "posts/blackassign0016.txt\n",
      "posts/blackassign0017.txt\n",
      "posts/blackassign0018.txt\n",
      "posts/blackassign0019.txt\n",
      "posts/blackassign0020.txt\n",
      "posts/blackassign0021.txt\n",
      "posts/blackassign0022.txt\n",
      "posts/blackassign0023.txt\n",
      "posts/blackassign0024.txt\n",
      "posts/blackassign0025.txt\n",
      "posts/blackassign0026.txt\n",
      "posts/blackassign0027.txt\n",
      "posts/blackassign0028.txt\n",
      "posts/blackassign0029.txt\n",
      "posts/blackassign0030.txt\n",
      "posts/blackassign0031.txt\n",
      "posts/blackassign0032.txt\n",
      "posts/blackassign0033.txt\n",
      "posts/blackassign0034.txt\n",
      "posts/blackassign0035.txt\n",
      "File not found: posts/blackassign0036.txt\n",
      "posts/blackassign0037.txt\n",
      "posts/blackassign0038.txt\n",
      "posts/blackassign0039.txt\n",
      "posts/blackassign0040.txt\n",
      "posts/blackassign0041.txt\n",
      "posts/blackassign0042.txt\n",
      "posts/blackassign0043.txt\n",
      "posts/blackassign0044.txt\n",
      "posts/blackassign0045.txt\n",
      "posts/blackassign0046.txt\n",
      "posts/blackassign0047.txt\n",
      "posts/blackassign0048.txt\n",
      "File not found: posts/blackassign0049.txt\n",
      "posts/blackassign0050.txt\n",
      "posts/blackassign0051.txt\n",
      "posts/blackassign0052.txt\n",
      "posts/blackassign0053.txt\n",
      "posts/blackassign0054.txt\n",
      "posts/blackassign0055.txt\n",
      "posts/blackassign0056.txt\n",
      "posts/blackassign0057.txt\n",
      "posts/blackassign0058.txt\n",
      "posts/blackassign0059.txt\n",
      "posts/blackassign0060.txt\n",
      "posts/blackassign0061.txt\n",
      "posts/blackassign0062.txt\n",
      "posts/blackassign0063.txt\n",
      "posts/blackassign0064.txt\n",
      "posts/blackassign0065.txt\n",
      "posts/blackassign0066.txt\n",
      "posts/blackassign0067.txt\n",
      "posts/blackassign0068.txt\n",
      "posts/blackassign0069.txt\n",
      "posts/blackassign0070.txt\n",
      "posts/blackassign0071.txt\n",
      "posts/blackassign0072.txt\n",
      "posts/blackassign0073.txt\n",
      "posts/blackassign0074.txt\n",
      "posts/blackassign0075.txt\n",
      "posts/blackassign0076.txt\n",
      "posts/blackassign0077.txt\n",
      "posts/blackassign0078.txt\n",
      "posts/blackassign0079.txt\n",
      "posts/blackassign0080.txt\n",
      "posts/blackassign0081.txt\n",
      "posts/blackassign0082.txt\n",
      "posts/blackassign0083.txt\n",
      "posts/blackassign0084.txt\n",
      "posts/blackassign0085.txt\n",
      "posts/blackassign0086.txt\n",
      "posts/blackassign0087.txt\n",
      "posts/blackassign0088.txt\n",
      "posts/blackassign0089.txt\n",
      "posts/blackassign0090.txt\n",
      "posts/blackassign0091.txt\n",
      "posts/blackassign0092.txt\n",
      "posts/blackassign0093.txt\n",
      "posts/blackassign0094.txt\n",
      "posts/blackassign0095.txt\n",
      "posts/blackassign0096.txt\n",
      "posts/blackassign0097.txt\n",
      "posts/blackassign0098.txt\n",
      "    POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE\n",
      "0             44.0             5.0        0.795918\n",
      "1             64.0            31.0        0.347368\n",
      "2             39.0            23.0        0.258065\n",
      "3             39.0            71.0       -0.290909\n",
      "4             22.0             8.0        0.466667\n",
      "..             ...             ...             ...\n",
      "93            34.0            47.0       -0.160494\n",
      "94            11.0            25.0       -0.388889\n",
      "95            30.0            55.0       -0.294118\n",
      "96            32.0            34.0       -0.030303\n",
      "97             6.0             2.0        0.500000\n",
      "\n",
      "[98 rows x 3 columns]\n",
      "DataFrame saved to output.csv \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# detect the type of enconding used in the file, ie utf-8 or something else\n",
    "def encoding_detect(fname):\n",
    "    file = open(fname, 'rb')\n",
    "    result = chardet.detect(file.read())\n",
    "    return result['encoding']\n",
    "    \n",
    "#read the filepath and return the text contents as list\n",
    "def read_file(filename):\n",
    "    wordfile = open(filename, 'r',encoding = encoding_detect(filename))\n",
    "    words = wordfile.read().split()\n",
    "    return words\n",
    "\n",
    "# count the number or syllabels\n",
    "def count_syllables(word):\n",
    "    vowels = \"aeiouy\"\n",
    "    count = 0\n",
    "    last_char = \"\"\n",
    "\n",
    "    for char in word:\n",
    "        if char.lower() in vowels and last_char.lower() not in vowels:\n",
    "            count += 1\n",
    "        last_char = char\n",
    "\n",
    "    # Handle exceptions for words ending with \"es\" and \"ed\"\n",
    "    if word.endswith((\"es\", \"ed\")) and len(word) > 2:\n",
    "        count -= 1\n",
    "\n",
    "    # Ensure at least one syllable for short words\n",
    "    return max(count, 1)  \n",
    "\n",
    "######### NOTE\n",
    "#specify the stopword directory \n",
    "stopword_directory = 'StopWords/'\n",
    "# Get full paths to stopword files\n",
    "stopword_files = [os.path.join(stopword_directory, f) for f in os.listdir(stopword_directory) if f.endswith(\".txt\")]\n",
    "stopwords = [] # empty stopwords list\n",
    "# Read each stopword file and add words to set, handling potential encoding errors\n",
    "for filename in stopword_files:\n",
    "    sw = open(filename, 'r',encoding = encoding_detect(filename))\n",
    "    stopwords.append(sw.read().strip().split())\n",
    "\n",
    "#specifly the path\n",
    "negative_words = read_file(\"MasterDictionary/negative-words.txt\")\n",
    "positive_words = read_file('MasterDictionary/positive-words.txt')\n",
    "\n",
    "#specify the text directory\n",
    "text_directory = 'posts/'\n",
    "text_files = [os.path.join(text_directory, f) for f in os.listdir(text_directory) if f.endswith(\".txt\")]\n",
    "data_list=[]\n",
    "\n",
    "for i in range(len(text_files)):\n",
    "    ## Reading words for the file, as they were saved\n",
    "    try:\n",
    "        with open(f'{text_directory}blackassign{i+1:04}.txt', 'r') as document:  # Use a context manager\n",
    "            doc = document.read()\n",
    "            words = word_tokenize(doc)\n",
    "            print(f'{text_directory}blackassign{i+1:04}.txt')\n",
    "            # Process the words as needed\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"File not found: {e.filename}\")\n",
    "        data_list.append({ \n",
    "    'POSITIVE SCORE': None,\n",
    "    'NEGATIVE SCORE': None,\n",
    "    'POLARITY SCORE': None,\n",
    "    \n",
    "    }\n",
    "        )\n",
    "        continue\n",
    "    sentences = sent_tokenize(doc)\n",
    "    \n",
    "    #fliter\n",
    "    filtered_words = [word for word in words if word.lower() not in stopwords]\n",
    "    words_count = len(filtered_words)\n",
    "    \n",
    "    positive_score = sum(1 for word in filtered_words if word in positive_words)\n",
    "    negative_score = sum(1 for word in filtered_words if word in negative_words)\n",
    "    \n",
    "    ##popularity score\n",
    "    polarity_score = (positive_score - negative_score) /((positive_score + negative_score) + 0.000001 ) \n",
    "    sentiment = \"Positive\" if positive_score > negative_score else \"Negative\"\n",
    "\n",
    "#creating a dictionary\n",
    "    raw_data = { \n",
    "    'POSITIVE SCORE': positive_score,\n",
    "    'NEGATIVE SCORE': negative_score,\n",
    "    'POLARITY SCORE': polarity_score,\n",
    "    \n",
    "    } # append the dictionary to the data_list list\n",
    "    data_list.append(raw_data)\n",
    "#df = df[[\"URL_ID\",\"URL\"]] # only considering the first two columns for the data frame,removing the empty ones\n",
    "mf = pd.DataFrame(data_list) # creating new dataframe for our new data\n",
    "combined_df = pd.concat([mf], axis=1) # combining the dataframe\n",
    "print(combined_df) \n",
    "combined_df.to_csv(\"output.csv\", index=False) # don't know why exporting to xslx is not working, so exported to csv\n",
    "print(f\"DataFrame saved to output.csv \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269e57a3-5745-44ed-9f96-4cfaa0223029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c7be42-16fd-4e1d-a4b1-95ab37f6cbac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ea79eb-fcf0-49e1-8445-7ea310c57fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
